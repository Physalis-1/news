{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77671324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import requests\n",
    "import urllib.request\n",
    "import copy\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "import re\n",
    "import nltk\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "import re \n",
    "path_save_real_news='real/'\n",
    "# словарь для отнесения к теме\n",
    "data = pd.read_csv('vocab/vocab.csv')\n",
    "stopwords_ru=list(data['0'])\n",
    "# символы для удаления\n",
    "patterns = \"[!#$%&'()*+,./:;<=>?@[\\]^_`{|}~—\\\"\\-]+\"\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8fa531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# новостей про ИБ с кждого сайта \n",
    "it_news_count=200\n",
    "# новостей не про ИБ с каждого сайта\n",
    "simpl_news_count=200\n",
    "# процент слов из словаря, который должна содержать статья\n",
    "percent=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ae40ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# лемматизация\n",
    "def lemmatize(doc):\n",
    "    global percent\n",
    "    doc = re.sub(patterns, ' ', doc)\n",
    "    tokens = []\n",
    "    for token in doc.split():\n",
    "        if token and token not in stopwords_ru:\n",
    "            token = token.strip()\n",
    "            token = morph.normal_forms(token)[0]\n",
    "            tokens.append(token)\n",
    "    raznost=len(doc.split())-len(tokens)\n",
    "    if  raznost >= round(percent*len(doc.split())):\n",
    "        return 0\n",
    "    else:\n",
    "        return 112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3513e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# получить текст новостной статьи и проверяем его на принадлежность теме\n",
    "# 0 - текст новости\n",
    "# 1 - относится ли к ИБ (1-да, 0- нет)\n",
    "# 2 - является фейком (1-да,0 - нет)\n",
    "# 3 - ссылка на новость\n",
    "def get_news(url, data_,flag,datc):\n",
    "    news = Article(url, language='ru')\n",
    "    news.download()\n",
    "    news.parse()\n",
    "    data_frame=pd.DataFrame()\n",
    "    el=str(news.text)\n",
    "    if flag==1:\n",
    "        text = copy.deepcopy(el[el.find(\"\\n\",el.find(\"\\n\")+2)+2:].lower())\n",
    "    else:\n",
    "        text=copy.deepcopy(el)\n",
    "    data_frame=pd.DataFrame()\n",
    "    data_frame=pd.concat([data_frame,pd.DataFrame([text])])\n",
    "    flag=data_frame[0].apply(lemmatize)\n",
    "    if (datc==0 or datc==1) and flag[0]==0:\n",
    "        data_=pd.concat([data_,pd.DataFrame({'News':[copy.deepcopy(el)], 'InfSec':[1],'FAKE?':[0],'Link':[copy.deepcopy(url)]})])\n",
    "        return data_,0\n",
    "    elif (datc==0 or datc==2) and flag[0]!=0:\n",
    "        data_=pd.concat([data_,pd.DataFrame({'News':[copy.deepcopy(el)], 'InfSec':[0],'FAKE?':[0],'Link':[copy.deepcopy(url)]})])\n",
    "        return data_,112\n",
    "    return None,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0398dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_func(flag,it_news,simpl_news,datchik):\n",
    "    #отображает какие новости добавлять: 0-все, 1-ИБ, 2-простые \n",
    "    #(нужно чтобы когда закончили добавлять один тип, продолжили искать другой)\n",
    "    global it_news_count\n",
    "    global simpl_news_count\n",
    "    if flag==0:\n",
    "        it_news=it_news+1\n",
    "    else:\n",
    "        simpl_news=simpl_news+1\n",
    "    if datchik==0:\n",
    "        if it_news>=it_news_count:\n",
    "            datchik=2\n",
    "        if simpl_news>=simpl_news_count:\n",
    "            datchik=1\n",
    "    return it_news,simpl_news,datchik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575f5d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# далее ддля каждого сайта из за различной разметки\n",
    "# _page - вынает количество страниц с сайта\n",
    "# _news - осуществляет изъятие текстов новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b974ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# www.securitylab.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb50179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# выясняем сколько страниц на сайие\n",
    "def secure_lab_page():\n",
    "    url=\"https://www.securitylab.ru/news/page1_1.php\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:100.0) Gecko/20100101 Firefox/100.0'}\n",
    "    result = requests.get(url, headers=headers)\n",
    "    # поиск по тегам\n",
    "    soup = BeautifulSoup(result.text, 'html.parser', parse_only=SoupStrainer('div'))\n",
    "    box = soup.find('div', class_='page-picker')\n",
    "    st = str(box.find('input'))\n",
    "    num_page=''\n",
    "    for i in range(12,len(st)):\n",
    "        if st[i]=='\"':\n",
    "            break\n",
    "        else:\n",
    "            num_page=num_page+st[i]\n",
    "    num_page=int(num_page)\n",
    "    return num_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b60c5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем новости\n",
    "def secure_lab_news(num_page):\n",
    "    global it_news_count\n",
    "    global simpl_news_count\n",
    "    it_news=0\n",
    "    simpl_news=0\n",
    "#     отображает какие новости добавлять: 0-все, 1-ИБ, 2-простые\n",
    "    datchik=0\n",
    "    dataframe=pd.DataFrame()\n",
    "    for k in range(1,num_page+1):\n",
    "        url=\"https://www.securitylab.ru/news/page1_\"+str(k)+\".php\"\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:100.0) Gecko/20100101 Firefox/100.0'}\n",
    "        result = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(result.text, 'html.parser', parse_only=SoupStrainer('div'))\n",
    "        box = soup.find_all('a', class_='article-card inline-card')\n",
    "        link=[]\n",
    "        for i in range (0, len(box)):\n",
    "            try:\n",
    "                stroka_url=str(box[i])\n",
    "                urls=''\n",
    "                for j in range (42,len(stroka_url)):\n",
    "                    if stroka_url[j]!='\"':\n",
    "                        urls=urls+stroka_url[j]\n",
    "                    else:\n",
    "                        temp_dataframe,flag=get_news(\"https://www.securitylab.ru\"+urls,dataframe,1,datchik)\n",
    "                        if flag!=None:\n",
    "                            dataframe=copy.deepcopy(temp_dataframe)\n",
    "                        break\n",
    "                if flag!=None:\n",
    "                    it_news,simpl_news,datchik=flag_func(flag,it_news,simpl_news,datchik)\n",
    "                    if it_news>=it_news_count and simpl_news>=simpl_news_count:\n",
    "                        print('len dataframe=',len(dataframe),' it_news=',it_news,' simpl_news=',simpl_news,\"  странициа сайта=\",k)\n",
    "                        return dataframe\n",
    "            except Exception:\n",
    "                e=1\n",
    "        print('len dataframe=',len(dataframe),' it_news=',it_news,' simpl_news=',simpl_news,\"  странициа сайта=\",k)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8466762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xakep.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6938f13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# выясняем сколько страниц на сайие\n",
    "def xakep_page():\n",
    "    url=\"https://xakep.ru/category/news/\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:100.0) Gecko/20100101 Firefox/100.0'}\n",
    "    result = requests.get(url, headers=headers)\n",
    "    # поиск по тегам\n",
    "    soup = BeautifulSoup(result.text, 'html.parser', parse_only=SoupStrainer('div'))\n",
    "    box = soup.find('span', class_='pages')\n",
    "    st=str(box)\n",
    "    num_page=''\n",
    "    for i in range(34,len(st)):\n",
    "        if st[i]=='<':\n",
    "            break\n",
    "        elif re.search(r'[0-9]', st[i]):\n",
    "            num_page=num_page+st[i]\n",
    "    num_page=int(num_page)\n",
    "    return num_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9753ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем новости\n",
    "def xakep_news(num_page):\n",
    "    global it_news_count\n",
    "    global simpl_news_count\n",
    "    it_news=0\n",
    "    simpl_news=0\n",
    "#     отображает какие новости добавлять: 0-все, 1-ИБ, 2-простые\n",
    "    datchik=0\n",
    "    dataframe=pd.DataFrame()\n",
    "    for k in range(1,num_page+1):\n",
    "        url=\"https://xakep.ru/category/news/page/\"+str(k)+\"/\"\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:100.0) Gecko/20100101 Firefox/100.0'}\n",
    "        result = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(result.text, 'html.parser', parse_only=SoupStrainer('div',class_='bdaia-blocks-container'))\n",
    "        box = soup.find_all('h3',class_='entry-title')\n",
    "        link=[]\n",
    "        for i in range (0, len(box)):\n",
    "            try:\n",
    "                stroka_url=str(box[i])\n",
    "                urls=''\n",
    "                for j in range (33,len(stroka_url)):\n",
    "                    if stroka_url[j]!='\"':\n",
    "                        urls=urls+stroka_url[j]\n",
    "                    else:\n",
    "                        temp_dataframe,flag=get_news(urls,dataframe,2,datchik)\n",
    "                        if flag!=None:\n",
    "                            dataframe=copy.deepcopy(temp_dataframe)\n",
    "                        break\n",
    "                if flag!=None:\n",
    "                    it_news,simpl_news,datchik=flag_func(flag,it_news,simpl_news,datchik)\n",
    "                    if it_news>=it_news_count and simpl_news>=simpl_news_count:\n",
    "                        print('len dataframe=',len(dataframe),' it_news=',it_news,' simpl_news=',simpl_news,\"  странициа сайта=\",k)\n",
    "                        return dataframe\n",
    "            except Exception:\n",
    "                e=1\n",
    "        print('len dataframe=',len(dataframe),' it_news=',it_news,' simpl_news=',simpl_news,\"  странициа сайта=\",k)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875c9e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# www.anti-malware.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e97dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# выясняем сколько страниц на сайие\n",
    "def anti_malware_page():\n",
    "    url=\"https://www.anti-malware.ru/news?page=0\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:100.0) Gecko/20100101 Firefox/100.0'}\n",
    "    result = requests.get(url, headers=headers)\n",
    "    # поиск по тегам\n",
    "    soup = BeautifulSoup(result.text, 'html.parser', parse_only=SoupStrainer('li',class_='pager-last'))\n",
    "    box = soup.find('a')\n",
    "    st=str(box)\n",
    "    num_page=''\n",
    "    for i in range(20,len(st)):\n",
    "        if st[i]=='\"':\n",
    "            break\n",
    "        elif re.search(r'[0-9]', st[i]):\n",
    "            num_page=num_page+st[i]\n",
    "    num_page=int(num_page)\n",
    "    return num_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82bbf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем новости\n",
    "def anti_malware_news(num_page):\n",
    "    global it_news_count\n",
    "    global simpl_news_count\n",
    "    it_news=0\n",
    "    simpl_news=0\n",
    "#     отображает какие новости добавлять: 0-все, 1-ИБ, 2-простые\n",
    "    datchik=0\n",
    "    dataframe=pd.DataFrame()\n",
    "    for k in range(0,num_page):\n",
    "        url=\"https://www.anti-malware.ru/news?page=\"+str(k)\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:100.0) Gecko/20100101 Firefox/100.0'}\n",
    "        result = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(result.text, 'html.parser', parse_only=SoupStrainer('div',class_='region region-content'))\n",
    "        box = soup.find_all('h2')\n",
    "        link=[]\n",
    "        for i in range (0, len(box)):\n",
    "            try:\n",
    "                stroka_url=str(box[i])\n",
    "                urls=''\n",
    "                for j in range (13,len(stroka_url)):\n",
    "                    if stroka_url[j]!='\"':\n",
    "                        urls=urls+stroka_url[j]\n",
    "                    else:\n",
    "                        temp_dataframe,flag=get_news('https://www.anti-malware.ru'+urls,dataframe,2,datchik)\n",
    "                        if flag!=None:\n",
    "                            dataframe=copy.deepcopy(temp_dataframe)\n",
    "                        break\n",
    "                if flag!=None:\n",
    "                    it_news,simpl_news,datchik=flag_func(flag,it_news,simpl_news,datchik)\n",
    "                    if it_news>=it_news_count and simpl_news>=simpl_news_count:\n",
    "                        print('len dataframe=',len(dataframe),' it_news=',it_news,' simpl_news=',simpl_news,\"  странициа сайта=\",k)\n",
    "                        return dataframe\n",
    "            except Exception:\n",
    "                e=1\n",
    "        print('len dataframe=',len(dataframe),' it_news=',it_news,' simpl_news=',simpl_news,\"  странициа сайта=\",k)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fb60f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# securitymedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd892bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# выясняем сколько страниц на сайие\n",
    "def securitymedia_page():\n",
    "    url=\"https://securitymedia.org/news/?PAGEN_2=1\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:100.0) Gecko/20100101 Firefox/100.0'}\n",
    "    result = requests.get(url, headers=headers)\n",
    "    # поиск по тегам\n",
    "    soup = BeautifulSoup(result.text, 'html.parser', parse_only=SoupStrainer('div',class_='modern-page-navigation'))\n",
    "    box = soup.find_all('a')\n",
    "    st=str(box[len(box)-2])\n",
    "    num_page=''\n",
    "    for i in range(24,len(st)):\n",
    "        if st[i]=='\"':\n",
    "            break\n",
    "        else:\n",
    "            num_page=num_page+st[i]\n",
    "    num_page=int(num_page)\n",
    "    return num_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c807f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем новости\n",
    "def securitymedia_news(num_page):\n",
    "    global it_news_count\n",
    "    global simpl_news_count\n",
    "    it_news=0\n",
    "    simpl_news=0\n",
    "#     отображает какие новости добавлять: 0-все, 1-ИБ, 2-простые\n",
    "    datchik=0\n",
    "    dataframe=pd.DataFrame()\n",
    "    for k in range(0,num_page):\n",
    "        url=\"https://securitymedia.org/news/?PAGEN_2=\"+str(k)\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:100.0) Gecko/20100101 Firefox/100.0'}\n",
    "        result = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(result.text, 'html.parser', parse_only=SoupStrainer('div',class_='col-md-9'))\n",
    "        box = soup.find_all('a',class_=\"text-dark\")\n",
    "        link=[]\n",
    "        for i in range (0, len(box),2):\n",
    "            try:\n",
    "                stroka_url=str(box[i])\n",
    "                urls=''\n",
    "                \n",
    "                for j in range (27,len(stroka_url)):\n",
    "                    if stroka_url[j]!='\"':\n",
    "                        urls=urls+stroka_url[j]\n",
    "                    else:\n",
    "                        temp_dataframe,flag=get_news('https://securitymedia.org'+urls,dataframe,2,datchik)\n",
    "                        if flag!=None:\n",
    "                            dataframe=copy.deepcopy(temp_dataframe)\n",
    "                        break\n",
    "                if flag!=None:\n",
    "                    it_news,simpl_news,datchik=flag_func(flag,it_news,simpl_news,datchik)\n",
    "                    if it_news>=it_news_count and simpl_news>=simpl_news_count:\n",
    "                        print('len dataframe=',len(dataframe),' it_news=',it_news,' simpl_news=',simpl_news,\"  странициа сайта=\",k)\n",
    "                        return dataframe\n",
    "            except Exception:\n",
    "                e=1\n",
    "        print('len dataframe=',len(dataframe),' it_news=',it_news,' simpl_news=',simpl_news,\"  странициа сайта=\",k)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfd1622",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data1frame=pd.DataFrame()\n",
    "result_data2frame=pd.DataFrame()\n",
    "result_data3frame=pd.DataFrame()\n",
    "result_data4frame=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae6d83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "page=securitymedia_page()\n",
    "site_pd=securitymedia_news(copy.deepcopy(page))\n",
    "result_data1frame=pd.concat([site_pd,result_data1frame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2757d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "page=anti_malware_page()\n",
    "# тут страницы с нулевой\n",
    "site_pd=anti_malware_news(copy.deepcopy(page+1))\n",
    "result_data2frame=pd.concat([site_pd,result_data2frame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205f2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "page=xakep_page()\n",
    "site_pd=xakep_news(copy.deepcopy(page))\n",
    "result_data3frame=pd.concat([site_pd,result_data3frame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc9c750",
   "metadata": {},
   "outputs": [],
   "source": [
    "page=secure_lab_page()\n",
    "site_pd=secure_lab_news(copy.deepcopy(page))\n",
    "result_data4frame=pd.concat([site_pd,result_data4frame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7b80b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data1frame.to_csv(path_save_real_news+'dataset1_news.csv', encoding='utf-8', index=False)\n",
    "result_data2frame.to_csv(path_save_real_news+'dataset2_news.csv', encoding='utf-8', index=False)\n",
    "result_data3frame.to_csv(path_save_real_news+'dataset3_news.csv', encoding='utf-8', index=False)\n",
    "result_data4frame.to_csv(path_save_real_news+'dataset4_news.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab01157",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataframe=pd.DataFrame()\n",
    "result_dataframe=pd.concat([result_dataframe,result_data1frame]).reset_index(drop=True)\n",
    "result_dataframe=pd.concat([result_dataframe,result_data2frame]).reset_index(drop=True)\n",
    "result_dataframe=pd.concat([result_dataframe,result_data3frame]).reset_index(drop=True)\n",
    "result_dataframe=pd.concat([result_dataframe,result_data4frame]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91ab7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_dataframe.to_csv('dataset_news.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e10a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(result_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8cde2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4488ca5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04336ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
