{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNxmXUpqppEBsA4FNjP+b6C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QgT6BxFiakTj","executionInfo":{"status":"ok","timestamp":1653863025983,"user_tz":-180,"elapsed":9156,"user":{"displayName":"Рубен Миронович","userId":"08191661292927132261"}},"outputId":"aff640dc-7c88-4924-d848-f932766aa064"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n"]}]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset\n","\n","class CustomDataset(Dataset):\n","\n","  def __init__(self, texts, targets, tokenizer, max_len=512):\n","    self.texts = texts\n","    self.targets = targets\n","    self.tokenizer = tokenizer\n","    self.max_len = max_len\n","\n","  def __len__(self):\n","    return len(self.texts)\n","\n","  def __getitem__(self, idx):\n","    text = str(self.texts[idx])\n","    target = self.targets[idx]\n","\n","    encoding = self.tokenizer.encode_plus(\n","        text,\n","        add_special_tokens=True,\n","        max_length=self.max_len,\n","        return_token_type_ids=False,\n","        padding='max_length',\n","        return_attention_mask=True,\n","        return_tensors='pt',\n","        truncation=True\n","    )\n","\n","    return {\n","      'text': text,\n","      'input_ids': encoding['input_ids'].flatten(),\n","      'attention_mask': encoding['attention_mask'].flatten(),\n","      'targets': torch.tensor(target, dtype=torch.long)\n","    }\n"],"metadata":{"id":"astyubzuaswy","executionInfo":{"status":"ok","timestamp":1653875156422,"user_tz":-180,"elapsed":3795,"user":{"displayName":"Рубен Миронович","userId":"08191661292927132261"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"t9geQ-tyadNo","executionInfo":{"status":"ok","timestamp":1653875162555,"user_tz":-180,"elapsed":4099,"user":{"displayName":"Рубен Миронович","userId":"08191661292927132261"}}},"outputs":[],"source":["import numpy as np\n","import torch\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","train_l = []\n","valid_l = []\n","train_a = []\n","valid_a = []\n","test_prec = []\n","test_rec = []\n","test_f1=[]\n","class BertClassifier:\n","    global train_l\n","    global valid_l \n","    global train_a \n","    global valid_a \n","    global test_prec \n","    global test_rec \n","    global test_f1\n","    def __init__(self, model_path, tokenizer_path, n_classes=2, epochs=1, model_save_path='/content/bert.pt'):\n","        self.model = BertForSequenceClassification.from_pretrained(model_path)\n","        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n","        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","        self.model_save_path=model_save_path\n","        self.max_len = 512\n","        self.epochs = epochs\n","        self.out_features = self.model.bert.encoder.layer[1].output.dense.out_features\n","        self.model.classifier = torch.nn.Linear(self.out_features, n_classes)\n","        self.model.to(self.device)\n","        self.train_loader=None\n","    \n","    def preparation(self, X_train, y_train, X_valid, y_valid):\n","        self.train_set = CustomDataset(X_train, y_train, self.tokenizer)\n","        self.valid_set = CustomDataset(X_valid, y_valid, self.tokenizer)\n","\n","        self.train_loader = DataLoader(self.train_set, batch_size=2, shuffle=True)\n","        self.valid_loader = DataLoader(self.valid_set, batch_size=2, shuffle=True)\n","\n","        self.optimizer = AdamW(self.model.parameters(), lr=2e-5, correct_bias=False)\n","        self.scheduler = get_linear_schedule_with_warmup(\n","                self.optimizer,\n","                num_warmup_steps=0,\n","                num_training_steps=len(self.train_loader) * self.epochs\n","            )\n","        self.loss_fn = torch.nn.CrossEntropyLoss().to(self.device)\n","            \n","    def fit(self):\n","        self.model = self.model.train()\n","        losses = []\n","        correct_predictions = 0\n","        lo=0\n","        for data in self.train_loader:\n","            lo=lo+2\n","            if lo%100==0:\n","                print('итерация ',lo,' из ',len(self.train_loader))\n","            input_ids = data[\"input_ids\"].to(self.device)\n","            attention_mask = data[\"attention_mask\"].to(self.device)\n","            targets = data[\"targets\"].to(self.device)\n","\n","            outputs = self.model(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask\n","                )\n","\n","            preds = torch.argmax(outputs.logits, dim=1)\n","            loss = self.loss_fn(outputs.logits, targets)\n","\n","            correct_predictions += torch.sum(preds == targets)\n","\n","            losses.append(loss.item())\n","            \n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n","            self.optimizer.step()\n","            self.scheduler.step()\n","            self.optimizer.zero_grad()\n","\n","        train_acc = correct_predictions.double() / len(self.train_set)\n","        train_loss = np.mean(losses)\n","        return train_acc, train_loss\n","    \n","    def eval(self):\n","        self.model = self.model.eval()\n","        losses = []\n","        correct_predictions = 0\n","\n","        with torch.no_grad():\n","            for data in self.valid_loader:\n","                input_ids = data[\"input_ids\"].to(self.device)\n","                attention_mask = data[\"attention_mask\"].to(self.device)\n","                targets = data[\"targets\"].to(self.device)\n","\n","                outputs = self.model(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask\n","                    )\n","\n","                preds = torch.argmax(outputs.logits, dim=1)\n","                loss = self.loss_fn(outputs.logits, targets)\n","                correct_predictions += torch.sum(preds == targets)\n","                losses.append(loss.item())\n","        \n","        val_acc = correct_predictions.double() / len(self.valid_set)\n","        val_loss = np.mean(losses)\n","        return val_acc, val_loss\n","    \n","    def train(self):\n","        import copy\n","        global train_l\n","        global valid_l \n","        global train_a \n","        global valid_a \n","        global test_prec \n","        global test_rec \n","        global test_f1\n","        best_accuracy = 0\n","        t_los=[1]\n","        # t_los=1\n","        for epoch in range(self.epochs):\n","            print(f'Epoch {epoch + 1}/{self.epochs}')\n","            train_acc, train_loss = self.fit()\n","            print(f'Train loss {train_loss} accuracy {train_acc}')\n","\n","            val_acc, val_loss = self.eval()\n","            print(f'Val loss {val_loss} accuracy {val_acc}')\n","            print('-' * 10)\n","            if (len(t_los)==2) and (train_loss>t_los[1] and train_loss>t_los[0]):\n","                # train_l.append(copy.deepcopy(train_loss))\n","                # train_a.append(copy.deepcopy(train_acc))\n","                # valid_l.append(copy.deepcopy(val_loss))\n","                # valid_a.append(copy.deepcopy(best_accuracy))\n","                break\n","            else:\n","                # torch.save(self.model, self.model_save_path)\n","                if len(t_los)==1:\n","                    t_los.append(copy.deepcopy(train_loss))\n","                else:\n","                    t_los.pop(0)\n","                    t_los.append(copy.deepcopy(train_loss))\n","            # if train_loss<t_los:\n","            #     torch.save(self.model, self.model_save_path)\n","            #     t_los=train_loss\n","            # else:\n","            #     break\n","            if val_acc > best_accuracy:\n","                torch.save(self.model, self.model_save_path)\n","                best_accuracy = val_acc\n","                train_l.append(copy.deepcopy(train_loss))\n","                train_a.append(copy.deepcopy(train_acc))\n","                valid_l.append(copy.deepcopy(val_loss))\n","                valid_a.append(copy.deepcopy(best_accuracy))\n","        self.model = torch.load(self.model_save_path)\n","    \n","    def predict(self, text):\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            return_token_type_ids=False,\n","            truncation=True,\n","            padding='max_length',\n","            return_attention_mask=True,\n","            return_tensors='pt',\n","        )\n","        \n","        out = {\n","              'text': text,\n","              'input_ids': encoding['input_ids'].flatten(),\n","              'attention_mask': encoding['attention_mask'].flatten()\n","          }\n","        \n","        input_ids = out[\"input_ids\"].to(self.device)\n","        attention_mask = out[\"attention_mask\"].to(self.device)\n","        \n","        outputs = self.model(\n","            input_ids=input_ids.unsqueeze(0),\n","            attention_mask=attention_mask.unsqueeze(0)\n","        )\n","        \n","        prediction = torch.argmax(outputs.logits, dim=1).cpu().numpy()[0]\n","\n","        return prediction\n"]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","import copy\n","import pandas as pd\n","dataframe=pd.read_csv('corrrect_dataset.csv')\n","dataframe.rename(columns={'News': 'text', 'FAKE?': 'label'}, inplace=True)\n","x=dataframe['text']\n","y=dataframe['label']\n","x1, x2, y1, y2 = train_test_split(x, y, train_size=0.8, random_state=10,stratify=y)\n","x_3, x_4, y_3, y_4 = train_test_split(x1, y1, train_size=0.5, random_state=10,stratify=y1)\n","x_tr1, x_te1, y_tr1, y_te1 = train_test_split(x_3, y_3, train_size=0.5, random_state=10,stratify=y_3)\n","x_tr2, x_te2, y_tr2, y_te2 = train_test_split(x_4, y_4, train_size=0.5, random_state=10,stratify=y_4)\n","elems=[]\n","s=pd.DataFrame()\n","# 1\n","te=pd.concat([pd.DataFrame(x2),pd.DataFrame(y2)],axis=1).reset_index(drop=True)\n","t0=pd.concat([pd.DataFrame(x_tr1),pd.DataFrame(y_tr1)],axis=1).reset_index(drop=True)\n","t1=pd.concat([pd.DataFrame(x_tr2),pd.DataFrame(y_tr2)],axis=1).reset_index(drop=True)\n","t2=pd.concat([pd.DataFrame(x_te1),pd.DataFrame(y_te1)],axis=1).reset_index(drop=True)\n","t3=pd.concat([pd.DataFrame(x_te2),pd.DataFrame(y_te2)],axis=1).reset_index(drop=True)\n","t4=pd.concat([t1,t2]).reset_index(drop=True)\n","t4=pd.concat([t4,t3]).reset_index(drop=True)\n","tr=pd.concat([t4,t0]).reset_index(drop=True)\n","tx,vx,ty,vy= train_test_split(tr['text'], tr['label'], train_size=0.8, random_state=10,stratify=tr['label'])\n","tr=pd.concat([pd.DataFrame(tx),pd.DataFrame(ty)],axis=1).reset_index(drop=True)\n","va=pd.concat([pd.DataFrame(vx),pd.DataFrame(vy)],axis=1).reset_index(drop=True)\n","elems.append([copy.deepcopy(tr),copy.deepcopy(te),copy.deepcopy(va)])\n","# # 2\n","# te=pd.concat([pd.DataFrame(x_tr1),pd.DataFrame(y_tr1)],axis=1).reset_index(drop=True)\n","# t0=pd.concat([pd.DataFrame(x2),pd.DataFrame(y2)],axis=1).reset_index(drop=True)\n","# t1=pd.concat([pd.DataFrame(x_tr2),pd.DataFrame(y_tr2)],axis=1).reset_index(drop=True)\n","# t2=pd.concat([pd.DataFrame(x_te1),pd.DataFrame(y_te1)],axis=1).reset_index(drop=True)\n","# t3=pd.concat([pd.DataFrame(x_te2),pd.DataFrame(y_te2)],axis=1).reset_index(drop=True)\n","# t4=pd.concat([t1,t2]).reset_index(drop=True)\n","# t4=pd.concat([t4,t3]).reset_index(drop=True)\n","# tr=pd.concat([t4,t0]).reset_index(drop=True)\n","# tx,vx,ty,vy= train_test_split(tr['text'], tr['label'], train_size=0.8, random_state=10,stratify=tr['label'])\n","# tr=pd.concat([pd.DataFrame(tx),pd.DataFrame(ty)],axis=1).reset_index(drop=True)\n","# va=pd.concat([pd.DataFrame(vx),pd.DataFrame(vy)],axis=1).reset_index(drop=True)\n","# elems.append([copy.deepcopy(tr),copy.deepcopy(te),copy.deepcopy(va)])\n","# # 3\n","# te=pd.concat([pd.DataFrame(x_tr2),pd.DataFrame(y_tr2)],axis=1).reset_index(drop=True)\n","# t0=pd.concat([pd.DataFrame(x2),pd.DataFrame(y2)],axis=1).reset_index(drop=True)\n","# t1=pd.concat([pd.DataFrame(x_tr1),pd.DataFrame(y_tr1)],axis=1).reset_index(drop=True)\n","# t2=pd.concat([pd.DataFrame(x_te1),pd.DataFrame(y_te1)],axis=1).reset_index(drop=True)\n","# t3=pd.concat([pd.DataFrame(x_te2),pd.DataFrame(y_te2)],axis=1).reset_index(drop=True)\n","# t4=pd.concat([t1,t2]).reset_index(drop=True)\n","# t4=pd.concat([t4,t3]).reset_index(drop=True)\n","# tr=pd.concat([t4,t0]).reset_index(drop=True)\n","# tx,vx,ty,vy= train_test_split(tr['text'], tr['label'], train_size=0.8, random_state=10,stratify=tr['label'])\n","# tr=pd.concat([pd.DataFrame(tx),pd.DataFrame(ty)],axis=1).reset_index(drop=True)\n","# va=pd.concat([pd.DataFrame(vx),pd.DataFrame(vy)],axis=1).reset_index(drop=True)\n","# elems.append([copy.deepcopy(tr),copy.deepcopy(te),copy.deepcopy(va)])\n","# # 4\n","# te=pd.concat([pd.DataFrame(x_te1),pd.DataFrame(y_te1)],axis=1).reset_index(drop=True)\n","# t0=pd.concat([pd.DataFrame(x2),pd.DataFrame(y2)],axis=1).reset_index(drop=True)\n","# t1=pd.concat([pd.DataFrame(x_tr2),pd.DataFrame(y_tr2)],axis=1).reset_index(drop=True)\n","# t2=pd.concat([pd.DataFrame(x_tr1),pd.DataFrame(y_tr1)],axis=1).reset_index(drop=True)\n","# t3=pd.concat([pd.DataFrame(x_te2),pd.DataFrame(y_te2)],axis=1).reset_index(drop=True)\n","# t4=pd.concat([t1,t2]).reset_index(drop=True)\n","# t4=pd.concat([t4,t3]).reset_index(drop=True)\n","# tr=pd.concat([t4,t0]).reset_index(drop=True)\n","# tx,vx,ty,vy= train_test_split(tr['text'], tr['label'], train_size=0.8, random_state=10,stratify=tr['label'])\n","# tr=pd.concat([pd.DataFrame(tx),pd.DataFrame(ty)],axis=1).reset_index(drop=True)\n","# va=pd.concat([pd.DataFrame(vx),pd.DataFrame(vy)],axis=1).reset_index(drop=True)\n","# elems.append([copy.deepcopy(tr),copy.deepcopy(te),copy.deepcopy(va)])\n","# # 5\n","# te=pd.concat([pd.DataFrame(x_te2),pd.DataFrame(y_te2)],axis=1).reset_index(drop=True)\n","# t0=pd.concat([pd.DataFrame(x2),pd.DataFrame(y2)],axis=1).reset_index(drop=True)\n","# t1=pd.concat([pd.DataFrame(x_tr2),pd.DataFrame(y_tr2)],axis=1).reset_index(drop=True)\n","# t2=pd.concat([pd.DataFrame(x_te1),pd.DataFrame(y_te1)],axis=1).reset_index(drop=True)\n","# t3=pd.concat([pd.DataFrame(x_tr1),pd.DataFrame(y_tr1)],axis=1).reset_index(drop=True)\n","# t4=pd.concat([t1,t2]).reset_index(drop=True)\n","# t4=pd.concat([t4,t3]).reset_index(drop=True)\n","# tr=pd.concat([t4,t0]).reset_index(drop=True)\n","# tx,vx,ty,vy= train_test_split(tr['text'], tr['label'], train_size=0.8, random_state=10,stratify=tr['label'])\n","# tr=pd.concat([pd.DataFrame(tx),pd.DataFrame(ty)],axis=1).reset_index(drop=True)\n","# va=pd.concat([pd.DataFrame(vx),pd.DataFrame(vy)],axis=1).reset_index(drop=True)\n","# elems.append([copy.deepcopy(tr),copy.deepcopy(te),copy.deepcopy(va)])\n","for i in range (0,len(elems)):\n","    train_data = elems[i][0]\n","    valid_data = elems[i][2]\n","    test_data  = elems[i][1]\n","    classifier = BertClassifier(\n","        model_path='cointegrated/rubert-tiny',\n","        tokenizer_path='cointegrated/rubert-tiny',\n","        n_classes=2,\n","        epochs=4,\n","        model_save_path='/content/bert.pt')\n","    classifier.preparation(\n","        X_train=list(train_data['text']),\n","        y_train=list(train_data['label']),\n","        X_valid=list(valid_data['text']),\n","        y_valid=list(valid_data['label']))\n","    classifier.train()\n","    texts = list(test_data['text'])\n","    labels = list(test_data['label'])\n","    predictions = [classifier.predict(t) for t in texts]\n","    from sklearn.metrics import precision_recall_fscore_support\n","    precision, recall, f1score = precision_recall_fscore_support(labels, predictions,average='macro')[:3]\n","    print(f'precision: {precision}, recall: {recall}, f1score: {f1score}')\n","    test_prec.append(copy.deepcopy(precision)) \n","    test_rec.append(copy.deepcopy(recall)) \n","    test_f1.append(copy.deepcopy(f1score))\n","    "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f4fMHObNT7vT","outputId":"04a793c3-4874-4573-aa24-d03eaabd741a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/4\n","итерация  100  из  256\n","итерация  200  из  256\n","итерация  300  из  256\n","итерация  400  из  256\n","итерация  500  из  256\n","Train loss 0.051035044979016675 accuracy 0.98046875\n","Val loss 0.0005440200079647184 accuracy 1.0\n","----------\n","Epoch 2/4\n","итерация  100  из  256\n","итерация  200  из  256\n","итерация  300  из  256\n","итерация  400  из  256\n","итерация  500  из  256\n","Train loss 0.00039198292830633363 accuracy 1.0\n","Val loss 0.00032755967549746856 accuracy 1.0\n","----------\n","Epoch 3/4\n","итерация  100  из  256\n","итерация  200  из  256\n","итерация  300  из  256\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from IPython.display import clear_output\n","from matplotlib.ticker import (MultipleLocator, AutoMinorLocator, LinearLocator, LogLocator)\n","import matplotlib.ticker as ticker\n","import numpy as np\n","fig, ax = plt.subplots()\n","fig.set_size_inches(8.5, 8.5)\n","ax.plot(train_l, label=\"train loss\")\n","ax.legend()\n","ax.plot(train_a, label=\"test accuracy\")\n","ax.legend()\n","ax.plot(valid_l, label=\"val loss\")\n","ax.legend()\n","ax.plot(valid_a, label=\"val accuracy\")\n","ax.legend()\n","# ax.plot(test_prec, label=\"precision\")\n","# ax.legend()\n","# ax.plot(test_rec, label=\"recall\")\n","# ax.legend()\n","ax.plot(test_f1, label=\"f1\")\n","ax.legend()\n","\n","ax.yaxis.set_major_locator(MultipleLocator(base=0.04))\n","plt.show()"],"metadata":{"id":"SSG1J79lT7mP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import pandas as pd\n","# train_data = pd.read_csv('train_dataset.csv')\n","# valid_data = pd.read_csv('valid_dataset.csv')\n","# test_data  = pd.read_csv('test_dataaset.csv')\n","# classifier = BertClassifier(\n","#         model_path='cointegrated/rubert-tiny',\n","#         tokenizer_path='cointegrated/rubert-tiny',\n","#         n_classes=2,\n","#         epochs=2,\n","#         model_save_path='/content/bert.pt'\n","# )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IbPUj0VsafIj","executionInfo":{"status":"ok","timestamp":1653842456080,"user_tz":-180,"elapsed":2603,"user":{"displayName":"Рубен Миронович","userId":"08191661292927132261"}},"outputId":"2b8d6caf-871b-4cf9-87fb-e4adfec75b93"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["# classifier.preparation(\n","#         X_train=list(train_data['text']),\n","#         y_train=list(train_data['label']),\n","#         X_valid=list(valid_data['text']),\n","#         y_valid=list(valid_data['label']))"],"metadata":{"id":"rybsdM8pviiU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# classifier.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jC5zzTTfafP6","executionInfo":{"status":"ok","timestamp":1653842951316,"user_tz":-180,"elapsed":494839,"user":{"displayName":"Рубен Миронович","userId":"08191661292927132261"}},"outputId":"3fa03c62-6b58-4ddd-eb69-ecc6a47a0809"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","итерация  100  из 480\n","итерация  200  из 480\n","итерация  300  из 480\n","итерация  400  из 480\n","Train loss 0.6720701104670297 accuracy 0.6916666666666667\n","Val loss 0.6012049832691749 accuracy 0.7416666666666667\n","----------\n","Epoch 2/2\n","итерация  100  из 480\n","итерация  200  из 480\n","итерация  300  из 480\n","итерация  400  из 480\n","Train loss 0.5025482886024596 accuracy 0.7895833333333333\n","Val loss 0.6097739484238749 accuracy 0.7666666666666667\n","----------\n"]}]},{"cell_type":"code","source":["# texts = list(test_data['text'])\n","# labels = list(test_data['label'])\n","# predictions = [classifier.predict(t) for t in texts]"],"metadata":{"id":"BiaXEh_FafS9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"lht4wU3NcajU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from sklearn.metrics import precision_recall_fscore_support\n","# precision, recall, f1score = precision_recall_fscore_support(labels, predictions,average='macro')[:3]\n","# print(f'precision: {precision}, recall: {recall}, f1score: {f1score}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pw-puBwnafVi","executionInfo":{"status":"ok","timestamp":1653842972862,"user_tz":-180,"elapsed":14,"user":{"displayName":"Рубен Миронович","userId":"08191661292927132261"}},"outputId":"1566fe7e-760e-4741-c8d5-3b8e01edb25a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["precision: 0.7071428571428572, recall: 0.7160551313093686, f1score: 0.7076023391812867\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"wiDYYA7oafYa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"2Wi-1Sswafay"},"execution_count":null,"outputs":[]}]}