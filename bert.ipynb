{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgT6BxFiakTj",
        "outputId": "c015e7b8-9205-44e8-caf4-6e0581f01ac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "  def __init__(self, texts, targets, tokenizer, max_len=512):\n",
        "    self.texts = texts\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    text = str(self.texts[idx])\n",
        "    target = self.targets[idx]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=self.max_len,\n",
        "        return_token_type_ids=False,\n",
        "        padding='max_length',\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'text': text,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "astyubzuaswy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9geQ-tyadNo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "train_l = []\n",
        "valid_l = []\n",
        "train_a = []\n",
        "valid_a = []\n",
        "test_prec = []\n",
        "test_rec = []\n",
        "test_f1=[]\n",
        "class BertClassifier:\n",
        "    global train_l\n",
        "    global valid_l \n",
        "    global train_a \n",
        "    global valid_a \n",
        "    global test_prec \n",
        "    global test_rec \n",
        "    global test_f1\n",
        "    def __init__(self, model_path, tokenizer_path, n_classes=2, epochs=1, model_save_path='/content/bert.pt'):\n",
        "        self.model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model_save_path=model_save_path\n",
        "        self.max_len = 512\n",
        "        self.epochs = epochs\n",
        "        self.out_features = self.model.bert.encoder.layer[1].output.dense.out_features\n",
        "        self.model.classifier = torch.nn.Linear(self.out_features, n_classes)\n",
        "        self.model.to(self.device)\n",
        "        self.train_loader=None\n",
        "    \n",
        "    def preparation(self, X_train, y_train, X_valid, y_valid):\n",
        "        self.train_set = CustomDataset(X_train, y_train, self.tokenizer)\n",
        "        self.valid_set = CustomDataset(X_valid, y_valid, self.tokenizer)\n",
        "\n",
        "        self.train_loader = DataLoader(self.train_set, batch_size=2, shuffle=True)\n",
        "        self.valid_loader = DataLoader(self.valid_set, batch_size=2, shuffle=True)\n",
        "\n",
        "        self.optimizer = AdamW(self.model.parameters(), lr=2e-5, correct_bias=False)\n",
        "        self.scheduler = get_linear_schedule_with_warmup(\n",
        "                self.optimizer,\n",
        "                num_warmup_steps=0,\n",
        "                num_training_steps=len(self.train_loader) * self.epochs\n",
        "            )\n",
        "        self.loss_fn = torch.nn.CrossEntropyLoss().to(self.device)\n",
        "            \n",
        "    def fit(self):\n",
        "        self.model = self.model.train()\n",
        "        losses = []\n",
        "        correct_predictions = 0\n",
        "        lo=0\n",
        "        for data in self.train_loader:\n",
        "            lo=lo+2\n",
        "            if lo%100==0:\n",
        "                print('итерация ',lo,' из ',len(self.train_loader))\n",
        "            input_ids = data[\"input_ids\"].to(self.device)\n",
        "            attention_mask = data[\"attention_mask\"].to(self.device)\n",
        "            targets = data[\"targets\"].to(self.device)\n",
        "\n",
        "            outputs = self.model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "                )\n",
        "\n",
        "            preds = torch.argmax(outputs.logits, dim=1)\n",
        "            loss = self.loss_fn(outputs.logits, targets)\n",
        "\n",
        "            correct_predictions += torch.sum(preds == targets)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            \n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "            self.optimizer.step()\n",
        "            self.scheduler.step()\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "        train_acc = correct_predictions.double() / len(self.train_set)\n",
        "        train_loss = np.mean(losses)\n",
        "        return train_acc, train_loss\n",
        "    \n",
        "    def eval(self):\n",
        "        self.model = self.model.eval()\n",
        "        losses = []\n",
        "        correct_predictions = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data in self.valid_loader:\n",
        "                input_ids = data[\"input_ids\"].to(self.device)\n",
        "                attention_mask = data[\"attention_mask\"].to(self.device)\n",
        "                targets = data[\"targets\"].to(self.device)\n",
        "\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask\n",
        "                    )\n",
        "\n",
        "                preds = torch.argmax(outputs.logits, dim=1)\n",
        "                loss = self.loss_fn(outputs.logits, targets)\n",
        "                correct_predictions += torch.sum(preds == targets)\n",
        "                losses.append(loss.item())\n",
        "        \n",
        "        val_acc = correct_predictions.double() / len(self.valid_set)\n",
        "        val_loss = np.mean(losses)\n",
        "        return val_acc, val_loss\n",
        "    \n",
        "    def train(self):\n",
        "        import copy\n",
        "        global train_l\n",
        "        global valid_l \n",
        "        global train_a \n",
        "        global valid_a \n",
        "        global test_prec \n",
        "        global test_rec \n",
        "        global test_f1\n",
        "        best_accuracy = 0\n",
        "        t_los=[1]\n",
        "        # t_los=1\n",
        "        for epoch in range(self.epochs):\n",
        "            print(f'Epoch {epoch + 1}/{self.epochs}')\n",
        "            train_acc, train_loss = self.fit()\n",
        "            print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "            val_acc, val_loss = self.eval()\n",
        "            print(f'Val loss {val_loss} accuracy {val_acc}')\n",
        "            print('-' * 10)\n",
        "            if (len(t_los)==2) and (train_loss>t_los[1] and train_loss>t_los[0]):\n",
        "                # train_l.append(copy.deepcopy(train_loss))\n",
        "                # train_a.append(copy.deepcopy(train_acc))\n",
        "                # valid_l.append(copy.deepcopy(val_loss))\n",
        "                # valid_a.append(copy.deepcopy(best_accuracy))\n",
        "                break\n",
        "            else:\n",
        "                # torch.save(self.model, self.model_save_path)\n",
        "                if len(t_los)==1:\n",
        "                    t_los.append(copy.deepcopy(train_loss))\n",
        "                else:\n",
        "                    t_los.pop(0)\n",
        "                    t_los.append(copy.deepcopy(train_loss))\n",
        "            # if train_loss<t_los:\n",
        "            #     torch.save(self.model, self.model_save_path)\n",
        "            #     t_los=train_loss\n",
        "            # else:\n",
        "            #     break\n",
        "            if val_acc > best_accuracy:\n",
        "                torch.save(self.model, self.model_save_path)\n",
        "                best_accuracy = val_acc\n",
        "                train_l.append(copy.deepcopy(train_loss))\n",
        "                train_a.append(copy.deepcopy(train_acc))\n",
        "                valid_l.append(copy.deepcopy(val_loss))\n",
        "                valid_a.append(copy.deepcopy(best_accuracy))\n",
        "        self.model = torch.load(self.model_save_path)\n",
        "    \n",
        "    def predict(self, text):\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        \n",
        "        out = {\n",
        "              'text': text,\n",
        "              'input_ids': encoding['input_ids'].flatten(),\n",
        "              'attention_mask': encoding['attention_mask'].flatten()\n",
        "          }\n",
        "        \n",
        "        input_ids = out[\"input_ids\"].to(self.device)\n",
        "        attention_mask = out[\"attention_mask\"].to(self.device)\n",
        "        \n",
        "        outputs = self.model(\n",
        "            input_ids=input_ids.unsqueeze(0),\n",
        "            attention_mask=attention_mask.unsqueeze(0)\n",
        "        )\n",
        "        \n",
        "        prediction = torch.argmax(outputs.logits, dim=1).cpu().numpy()[0]\n",
        "\n",
        "        return prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import copy\n",
        "import pandas as pd\n",
        "dataframe=pd.read_csv('corrrect_dataset.csv')\n",
        "dataframe.rename(columns={'News': 'text', 'FAKE?': 'label'}, inplace=True)\n",
        "x=dataframe['text']\n",
        "y=dataframe['label']\n",
        "x1, x2, y1, y2 = train_test_split(x, y, train_size=0.8, random_state=10,stratify=y)\n",
        "x_3, x_4, y_3, y_4 = train_test_split(x1, y1, train_size=0.5, random_state=10,stratify=y1)\n",
        "x_tr1, x_te1, y_tr1, y_te1 = train_test_split(x_3, y_3, train_size=0.5, random_state=10,stratify=y_3)\n",
        "x_tr2, x_te2, y_tr2, y_te2 = train_test_split(x_4, y_4, train_size=0.5, random_state=10,stratify=y_4)\n",
        "elems=[]\n",
        "s=pd.DataFrame()\n",
        "# 1\n",
        "te=pd.concat([pd.DataFrame(x2),pd.DataFrame(y2)],axis=1).reset_index(drop=True)\n",
        "t0=pd.concat([pd.DataFrame(x_tr1),pd.DataFrame(y_tr1)],axis=1).reset_index(drop=True)\n",
        "t1=pd.concat([pd.DataFrame(x_tr2),pd.DataFrame(y_tr2)],axis=1).reset_index(drop=True)\n",
        "t2=pd.concat([pd.DataFrame(x_te1),pd.DataFrame(y_te1)],axis=1).reset_index(drop=True)\n",
        "t3=pd.concat([pd.DataFrame(x_te2),pd.DataFrame(y_te2)],axis=1).reset_index(drop=True)\n",
        "t4=pd.concat([t1,t2]).reset_index(drop=True)\n",
        "t4=pd.concat([t4,t3]).reset_index(drop=True)\n",
        "tr=pd.concat([t4,t0]).reset_index(drop=True)\n",
        "tx,vx,ty,vy= train_test_split(tr['text'], tr['label'], train_size=0.8, random_state=10,stratify=tr['label'])\n",
        "tr=pd.concat([pd.DataFrame(tx),pd.DataFrame(ty)],axis=1).reset_index(drop=True)\n",
        "va=pd.concat([pd.DataFrame(vx),pd.DataFrame(vy)],axis=1).reset_index(drop=True)\n",
        "elems.append([copy.deepcopy(tr),copy.deepcopy(te),copy.deepcopy(va)])\n",
        "# # 2\n",
        "# te=pd.concat([pd.DataFrame(x_tr1),pd.DataFrame(y_tr1)],axis=1).reset_index(drop=True)\n",
        "# t0=pd.concat([pd.DataFrame(x2),pd.DataFrame(y2)],axis=1).reset_index(drop=True)\n",
        "# t1=pd.concat([pd.DataFrame(x_tr2),pd.DataFrame(y_tr2)],axis=1).reset_index(drop=True)\n",
        "# t2=pd.concat([pd.DataFrame(x_te1),pd.DataFrame(y_te1)],axis=1).reset_index(drop=True)\n",
        "# t3=pd.concat([pd.DataFrame(x_te2),pd.DataFrame(y_te2)],axis=1).reset_index(drop=True)\n",
        "# t4=pd.concat([t1,t2]).reset_index(drop=True)\n",
        "# t4=pd.concat([t4,t3]).reset_index(drop=True)\n",
        "# tr=pd.concat([t4,t0]).reset_index(drop=True)\n",
        "# tx,vx,ty,vy= train_test_split(tr['text'], tr['label'], train_size=0.8, random_state=10,stratify=tr['label'])\n",
        "# tr=pd.concat([pd.DataFrame(tx),pd.DataFrame(ty)],axis=1).reset_index(drop=True)\n",
        "# va=pd.concat([pd.DataFrame(vx),pd.DataFrame(vy)],axis=1).reset_index(drop=True)\n",
        "# elems.append([copy.deepcopy(tr),copy.deepcopy(te),copy.deepcopy(va)])\n",
        "# # 3\n",
        "# te=pd.concat([pd.DataFrame(x_tr2),pd.DataFrame(y_tr2)],axis=1).reset_index(drop=True)\n",
        "# t0=pd.concat([pd.DataFrame(x2),pd.DataFrame(y2)],axis=1).reset_index(drop=True)\n",
        "# t1=pd.concat([pd.DataFrame(x_tr1),pd.DataFrame(y_tr1)],axis=1).reset_index(drop=True)\n",
        "# t2=pd.concat([pd.DataFrame(x_te1),pd.DataFrame(y_te1)],axis=1).reset_index(drop=True)\n",
        "# t3=pd.concat([pd.DataFrame(x_te2),pd.DataFrame(y_te2)],axis=1).reset_index(drop=True)\n",
        "# t4=pd.concat([t1,t2]).reset_index(drop=True)\n",
        "# t4=pd.concat([t4,t3]).reset_index(drop=True)\n",
        "# tr=pd.concat([t4,t0]).reset_index(drop=True)\n",
        "# tx,vx,ty,vy= train_test_split(tr['text'], tr['label'], train_size=0.8, random_state=10,stratify=tr['label'])\n",
        "# tr=pd.concat([pd.DataFrame(tx),pd.DataFrame(ty)],axis=1).reset_index(drop=True)\n",
        "# va=pd.concat([pd.DataFrame(vx),pd.DataFrame(vy)],axis=1).reset_index(drop=True)\n",
        "# elems.append([copy.deepcopy(tr),copy.deepcopy(te),copy.deepcopy(va)])\n",
        "# # 4\n",
        "# te=pd.concat([pd.DataFrame(x_te1),pd.DataFrame(y_te1)],axis=1).reset_index(drop=True)\n",
        "# t0=pd.concat([pd.DataFrame(x2),pd.DataFrame(y2)],axis=1).reset_index(drop=True)\n",
        "# t1=pd.concat([pd.DataFrame(x_tr2),pd.DataFrame(y_tr2)],axis=1).reset_index(drop=True)\n",
        "# t2=pd.concat([pd.DataFrame(x_tr1),pd.DataFrame(y_tr1)],axis=1).reset_index(drop=True)\n",
        "# t3=pd.concat([pd.DataFrame(x_te2),pd.DataFrame(y_te2)],axis=1).reset_index(drop=True)\n",
        "# t4=pd.concat([t1,t2]).reset_index(drop=True)\n",
        "# t4=pd.concat([t4,t3]).reset_index(drop=True)\n",
        "# tr=pd.concat([t4,t0]).reset_index(drop=True)\n",
        "# tx,vx,ty,vy= train_test_split(tr['text'], tr['label'], train_size=0.8, random_state=10,stratify=tr['label'])\n",
        "# tr=pd.concat([pd.DataFrame(tx),pd.DataFrame(ty)],axis=1).reset_index(drop=True)\n",
        "# va=pd.concat([pd.DataFrame(vx),pd.DataFrame(vy)],axis=1).reset_index(drop=True)\n",
        "# elems.append([copy.deepcopy(tr),copy.deepcopy(te),copy.deepcopy(va)])\n",
        "# # 5\n",
        "# te=pd.concat([pd.DataFrame(x_te2),pd.DataFrame(y_te2)],axis=1).reset_index(drop=True)\n",
        "# t0=pd.concat([pd.DataFrame(x2),pd.DataFrame(y2)],axis=1).reset_index(drop=True)\n",
        "# t1=pd.concat([pd.DataFrame(x_tr2),pd.DataFrame(y_tr2)],axis=1).reset_index(drop=True)\n",
        "# t2=pd.concat([pd.DataFrame(x_te1),pd.DataFrame(y_te1)],axis=1).reset_index(drop=True)\n",
        "# t3=pd.concat([pd.DataFrame(x_tr1),pd.DataFrame(y_tr1)],axis=1).reset_index(drop=True)\n",
        "# t4=pd.concat([t1,t2]).reset_index(drop=True)\n",
        "# t4=pd.concat([t4,t3]).reset_index(drop=True)\n",
        "# tr=pd.concat([t4,t0]).reset_index(drop=True)\n",
        "# tx,vx,ty,vy= train_test_split(tr['text'], tr['label'], train_size=0.8, random_state=10,stratify=tr['label'])\n",
        "# tr=pd.concat([pd.DataFrame(tx),pd.DataFrame(ty)],axis=1).reset_index(drop=True)\n",
        "# va=pd.concat([pd.DataFrame(vx),pd.DataFrame(vy)],axis=1).reset_index(drop=True)\n",
        "# elems.append([copy.deepcopy(tr),copy.deepcopy(te),copy.deepcopy(va)])\n",
        "for i in range (0,len(elems)):\n",
        "    train_data = elems[i][0]\n",
        "    valid_data = elems[i][2]\n",
        "    test_data  = elems[i][1]\n",
        "    classifier = BertClassifier(\n",
        "        model_path='cointegrated/rubert-tiny2',\n",
        "        tokenizer_path='cointegrated/rubert-tiny2',\n",
        "        n_classes=2,\n",
        "        epochs=1,\n",
        "        model_save_path='/content/bert.pt')\n",
        "    classifier.preparation(\n",
        "        X_train=list(train_data['text']),\n",
        "        y_train=list(train_data['label']),\n",
        "        X_valid=list(valid_data['text']),\n",
        "        y_valid=list(valid_data['label']))\n",
        "    classifier.train()\n",
        "    texts = list(test_data['text'])\n",
        "    labels = list(test_data['label'])\n",
        "    predictions = [classifier.predict(t) for t in texts]\n",
        "    from sklearn.metrics import precision_recall_fscore_support\n",
        "    precision, recall, f1score = precision_recall_fscore_support(labels, predictions,average='macro')[:3]\n",
        "    print(f'precision: {precision}, recall: {recall}, f1score: {f1score}')\n",
        "    test_prec.append(copy.deepcopy(precision)) \n",
        "    test_rec.append(copy.deepcopy(recall)) \n",
        "    test_f1.append(copy.deepcopy(f1score))\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "f4fMHObNT7vT",
        "outputId": "8252865d-08ec-40ae-cc26-905246c80179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-b578cbc518df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdataframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'corrrect_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'News'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FAKE?'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'corrrect_dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# from IPython.display import clear_output\n",
        "# from matplotlib.ticker import (MultipleLocator, AutoMinorLocator, LinearLocator, LogLocator)\n",
        "# import matplotlib.ticker as ticker\n",
        "# import numpy as np\n",
        "# fig, ax = plt.subplots()\n",
        "# fig.set_size_inches(8.5, 8.5)\n",
        "# ax.plot(train_l, label=\"train loss\")\n",
        "# ax.legend()\n",
        "# ax.plot(train_a, label=\"test accuracy\")\n",
        "# ax.legend()\n",
        "# ax.plot(valid_l, label=\"val loss\")\n",
        "# ax.legend()\n",
        "# ax.plot(valid_a, label=\"val accuracy\")\n",
        "# ax.legend()\n",
        "# # ax.plot(test_prec, label=\"precision\")\n",
        "# # ax.legend()\n",
        "# # ax.plot(test_rec, label=\"recall\")\n",
        "# # ax.legend()\n",
        "# ax.plot(test_f1, label=\"f1\")\n",
        "# ax.legend()\n",
        "\n",
        "# ax.yaxis.set_major_locator(MultipleLocator(base=0.04))\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "SSG1J79lT7mP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# train_data = pd.read_csv('train_dataset.csv')\n",
        "# valid_data = pd.read_csv('valid_dataset.csv')\n",
        "# test_data  = pd.read_csv('test_dataaset.csv')\n",
        "# classifier = BertClassifier(\n",
        "#         model_path='cointegrated/rubert-tiny',\n",
        "#         tokenizer_path='cointegrated/rubert-tiny',\n",
        "#         n_classes=2,\n",
        "#         epochs=2,\n",
        "#         model_save_path='/content/bert.pt'\n",
        "# )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbPUj0VsafIj",
        "outputId": "2b8d6caf-871b-4cf9-87fb-e4adfec75b93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# classifier.preparation(\n",
        "#         X_train=list(train_data['text']),\n",
        "#         y_train=list(train_data['label']),\n",
        "#         X_valid=list(valid_data['text']),\n",
        "#         y_valid=list(valid_data['label']))"
      ],
      "metadata": {
        "id": "rybsdM8pviiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classifier.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jC5zzTTfafP6",
        "outputId": "3fa03c62-6b58-4ddd-eb69-ecc6a47a0809"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "итерация  100  из 480\n",
            "итерация  200  из 480\n",
            "итерация  300  из 480\n",
            "итерация  400  из 480\n",
            "Train loss 0.6720701104670297 accuracy 0.6916666666666667\n",
            "Val loss 0.6012049832691749 accuracy 0.7416666666666667\n",
            "----------\n",
            "Epoch 2/2\n",
            "итерация  100  из 480\n",
            "итерация  200  из 480\n",
            "итерация  300  из 480\n",
            "итерация  400  из 480\n",
            "Train loss 0.5025482886024596 accuracy 0.7895833333333333\n",
            "Val loss 0.6097739484238749 accuracy 0.7666666666666667\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# texts = list(test_data['text'])\n",
        "# labels = list(test_data['label'])\n",
        "# predictions = [classifier.predict(t) for t in texts]"
      ],
      "metadata": {
        "id": "BiaXEh_FafS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lht4wU3NcajU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import precision_recall_fscore_support\n",
        "# precision, recall, f1score = precision_recall_fscore_support(labels, predictions,average='macro')[:3]\n",
        "# print(f'precision: {precision}, recall: {recall}, f1score: {f1score}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pw-puBwnafVi",
        "outputId": "1566fe7e-760e-4741-c8d5-3b8e01edb25a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: 0.7071428571428572, recall: 0.7160551313093686, f1score: 0.7076023391812867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wiDYYA7oafYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2Wi-1Sswafay"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
