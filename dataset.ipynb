{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "c8e6bcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import requests\n",
    "import urllib.request\n",
    "import copy\n",
    "import pandas as pd\n",
    "from newspaper import Article\n",
    "import re\n",
    "import nltk\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "import re \n",
    "data = pd.read_csv('vocab.csv')\n",
    "stopwords_ru=list(data['0'])\n",
    "# символы для удаления\n",
    "patterns = \"[!#$%&'()*+,./:;<=>?@[\\]^_`{|}~—\\\"\\-]+\"\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "5e6a56c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# лемматизация\n",
    "def lemmatize(doc):\n",
    "    doc = re.sub(patterns, ' ', doc)\n",
    "    tokens = []\n",
    "#     print(\"len=\",len(doc.split()))\n",
    "    for token in doc.split():\n",
    "        if token and token not in stopwords_ru:\n",
    "            token = token.strip()\n",
    "            token = morph.normal_forms(token)[0]\n",
    "            tokens.append(token)\n",
    "    raznost=len(doc.split())-len(tokens)\n",
    "#     print(\"/////////////////////////////////////\")\n",
    "#     print(raznost)\n",
    "#     print(round(0.07*len(doc.split())))\n",
    "#     print()\n",
    "    if  raznost >= round(0.1*len(doc.split())):\n",
    "        return 0\n",
    "    else:\n",
    "        return 112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "20f228d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# получить текст новостной статьи и проверяем его на принадлежность теме\n",
    "def get_news(url, data_,flag):\n",
    "    news = Article(url, language='ru')\n",
    "    news.download()\n",
    "    news.parse()\n",
    "    data_frame=pd.DataFrame()\n",
    "    el=str(news.text)\n",
    "    if flag==1:\n",
    "        text = copy.deepcopy(el[el.find(\"\\n\",el.find(\"\\n\")+2)+2:].lower())\n",
    "    else:\n",
    "        text=copy.deepcopy(el)\n",
    "    data_frame=pd.DataFrame()\n",
    "    data_frame=pd.concat([data_frame,pd.DataFrame([text])])\n",
    "    flag=data_frame[0].apply(lemmatize)\n",
    "    if flag[0]==0:\n",
    "        data_=pd.concat([data_,pd.DataFrame({'News':[copy.deepcopy(el)], 'InfSec':[1],'FAKE?':[0],'Link':[copy.deepcopy(url)]})])\n",
    "#     else:\n",
    "#         data_=pd.concat([data_,pd.DataFrame({'News':[copy.deepcopy(el)], 'InfSec':[0],'FAKE?':[0],'Link':[copy.deepcopy(url)]})])\n",
    "    return data_\n",
    "# 0 - текст новости\n",
    "# 1 - относится ли к ИБ (1-да, 0- нет)\n",
    "# 2 - является фейком (1-да,0 - нет)\n",
    "# 3 - ссылка на новость"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "2c8bed5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# выясняем сколько страниц на сайие\n",
    "def secure_lab_page():\n",
    "    url=\"https://www.securitylab.ru/news/page1_1.php\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:100.0) Gecko/20100101 Firefox/100.0'}\n",
    "    result = requests.get(url, headers=headers)\n",
    "    # поиск по тегам\n",
    "    soup = BeautifulSoup(result.text, 'html.parser', parse_only=SoupStrainer('div'))\n",
    "    box = soup.find('div', class_='page-picker')\n",
    "    st = str(box.find('input'))\n",
    "    num_page=''\n",
    "    for i in range(12,len(st)):\n",
    "        if st[i]=='\"':\n",
    "            break\n",
    "        else:\n",
    "            num_page=num_page+st[i]\n",
    "    num_page=int(num_page)\n",
    "    return num_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "5448dc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем новости\n",
    "def secure_lab_news(num_page):\n",
    "    dataframe=pd.DataFrame()\n",
    "    for k in range(1,num_page+1):\n",
    "        url=\"https://www.securitylab.ru/news/page1_\"+str(k)+\".php\"\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:100.0) Gecko/20100101 Firefox/100.0'}\n",
    "        result = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(result.text, 'html.parser', parse_only=SoupStrainer('div'))\n",
    "        box = soup.find_all('a', class_='article-card inline-card')\n",
    "        link=[]\n",
    "        for i in range (0, len(box)):\n",
    "            try:\n",
    "                stroka_url=str(box[i])\n",
    "                urls=''\n",
    "                for j in range (42,len(stroka_url)):\n",
    "                    if stroka_url[j]!='\"':\n",
    "                        urls=urls+stroka_url[j]\n",
    "                    else:\n",
    "                        dataframe=get_news(\"https://www.securitylab.ru\"+urls,dataframe,1)\n",
    "                        break\n",
    "                if len(dataframe)>=20:\n",
    "                    break\n",
    "            except ValueError:\n",
    "                e=1\n",
    "        if len(dataframe)>=20:\n",
    "                break\n",
    "        print(len(dataframe),\" k=\",k)\n",
    "    dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "28b6d236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# выясняем сколько страниц на сайие\n",
    "def xakep_page():\n",
    "    url=\"https://xakep.ru/category/news/\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:100.0) Gecko/20100101 Firefox/100.0'}\n",
    "    result = requests.get(url, headers=headers)\n",
    "    # поиск по тегам\n",
    "    soup = BeautifulSoup(result.text, 'html.parser', parse_only=SoupStrainer('div'))\n",
    "    box = soup.find('span', class_='pages')\n",
    "    st=str(box)\n",
    "    num_page=''\n",
    "    for i in range(34,len(st)):\n",
    "        if st[i]=='<':\n",
    "            break\n",
    "        elif re.search(r'[0-9]', st[i]):\n",
    "            num_page=num_page+st[i]\n",
    "    num_page=int(num_page)\n",
    "    return num_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "ec352011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xakep_news(num_page):\n",
    "    dataframe=pd.DataFrame()\n",
    "    for k in range(1,num_page+1):\n",
    "        url=\"https://xakep.ru/category/news/page/\"+str(k)+\"/\"\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:100.0) Gecko/20100101 Firefox/100.0'}\n",
    "        result = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(result.text, 'html.parser', parse_only=SoupStrainer('div',class_='bdaia-blocks-container'))\n",
    "        box = soup.find_all('h3',class_='entry-title')\n",
    "        link=[]\n",
    "        for i in range (0, len(box)):\n",
    "            try:\n",
    "                stroka_url=str(box[i])\n",
    "                urls=''\n",
    "                for j in range (33,len(stroka_url)):\n",
    "                    if stroka_url[j]!='\"':\n",
    "                        urls=urls+stroka_url[j]\n",
    "                    else:\n",
    "                        dataframe=get_news(urls,dataframe,2)\n",
    "                        break\n",
    "                if len(dataframe)>=20:\n",
    "                    break\n",
    "            except ValueError:\n",
    "                e=1\n",
    "        if len(dataframe)>=20:\n",
    "                break\n",
    "        print(len(dataframe),\" k=\",k)\n",
    "    dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "b3634873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# выясняем сколько страниц на сайие\n",
    "def anti_malware_page():\n",
    "    url=\"https://www.anti-malware.ru/news?page=0\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:100.0) Gecko/20100101 Firefox/100.0'}\n",
    "    result = requests.get(url, headers=headers)\n",
    "    # поиск по тегам\n",
    "    soup = BeautifulSoup(result.text, 'html.parser', parse_only=SoupStrainer('li',class_='pager-last'))\n",
    "    box = soup.find('a')\n",
    "    st=str(box)\n",
    "    num_page=''\n",
    "    for i in range(20,len(st)):\n",
    "        if st[i]=='\"':\n",
    "            break\n",
    "        elif re.search(r'[0-9]', st[i]):\n",
    "            num_page=num_page+st[i]\n",
    "    num_page=int(num_page)\n",
    "    return num_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "70bbe36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anti_malware_news(num_page):\n",
    "    dataframe=pd.DataFrame()\n",
    "    for k in range(0,num_page):\n",
    "        url=\"https://www.anti-malware.ru/news?page=\"+str(k)\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:100.0) Gecko/20100101 Firefox/100.0'}\n",
    "        result = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(result.text, 'html.parser', parse_only=SoupStrainer('div',class_='region region-content'))\n",
    "        box = soup.find_all('h2')\n",
    "        link=[]\n",
    "        for i in range (0, len(box)):\n",
    "            try:\n",
    "                stroka_url=str(box[i])\n",
    "                urls=''\n",
    "                for j in range (13,len(stroka_url)):\n",
    "                    if stroka_url[j]!='\"':\n",
    "                        urls=urls+stroka_url[j]\n",
    "                    else:\n",
    "                        dataframe=get_news('https://www.anti-malware.ru'+urls,dataframe,2)\n",
    "                        break\n",
    "                if len(dataframe)>=20:\n",
    "                    break\n",
    "            except ValueError:\n",
    "                e=1\n",
    "        if len(dataframe)>=20:\n",
    "                break\n",
    "        print(len(dataframe),\" k=\",k)\n",
    "    dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "158a221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# выясняем сколько страниц на сайие\n",
    "def securitymedia_page():\n",
    "    url=\"https://securitymedia.org/news/?PAGEN_2=1\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:100.0) Gecko/20100101 Firefox/100.0'}\n",
    "    result = requests.get(url, headers=headers)\n",
    "    # поиск по тегам\n",
    "    soup = BeautifulSoup(result.text, 'html.parser', parse_only=SoupStrainer('div',class_='modern-page-navigation'))\n",
    "    box = soup.find_all('a')\n",
    "    st=str(box[len(box)-2])\n",
    "    num_page=''\n",
    "    for i in range(24,len(st)):\n",
    "        if st[i]=='\"':\n",
    "            break\n",
    "        else:\n",
    "            num_page=num_page+st[i]\n",
    "#     print(num_page)\n",
    "    num_page=int(num_page)\n",
    "    return num_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "ede7a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def securitymedia_news(num_page):\n",
    "    dataframe=pd.DataFrame()\n",
    "    for k in range(0,num_page):\n",
    "        url=\"https://securitymedia.org/news/?PAGEN_2=\"+str(k)\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:100.0) Gecko/20100101 Firefox/100.0'}\n",
    "        result = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(result.text, 'html.parser', parse_only=SoupStrainer('div',class_='col-md-9'))\n",
    "        box = soup.find_all('a',class_=\"text-dark\")\n",
    "        link=[]\n",
    "        for i in range (0, len(box),2):\n",
    "            try:\n",
    "                stroka_url=str(box[i])\n",
    "                urls=''\n",
    "\n",
    "                for j in range (27,len(stroka_url)):\n",
    "                    if stroka_url[j]!='\"':\n",
    "                        urls=urls+stroka_url[j]\n",
    "                    else:\n",
    "                        dataframe=get_news('https://securitymedia.org'+urls,dataframe,2)\n",
    "                        break\n",
    "                if len(dataframe)>=20:\n",
    "                    break\n",
    "            except ValueError:\n",
    "                e=1\n",
    "        if len(dataframe)>=20:\n",
    "                break\n",
    "        print(len(dataframe),\" k=\",k)\n",
    "    dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "e1dc52ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "page=securitymedia_page()\n",
    "securitymedia_news(copy.deepcopy(page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0c26c2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "page=anti_malware_page()\n",
    "anti_malware_news(copy.deepcopy(page+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2840dabd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "page=xakep_page()\n",
    "xakep_news(copy.deepcopy(page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "739d5276",
   "metadata": {},
   "outputs": [],
   "source": [
    "page=secure_lab_page()\n",
    "secure_lab_news(copy.deepcopy(page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a947f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba721c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd20786",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
